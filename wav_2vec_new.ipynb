{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrQK05HAoO1eZYCS2gIsc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravidu-hevaganinge/AI_Scribe/blob/main/wav_2vec_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Libraries and prepare co-lab envirement\n"
      ],
      "metadata": {
        "id": "sCE1NWPKdxKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this section of code should be run at very of beginning!!\n"
      ],
      "metadata": {
        "id": "-t5t36HYjbyf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3bkbL5xcTsi",
        "outputId": "c69f572e-3346-4465-af51-74cba179f20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.9)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.12.25)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.2)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.25.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio fairseq #Install Required Libraries\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Load pre-trained model(wav2vec2) and processor\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3yjjP-pc2MO",
        "outputId": "266fa6b3-46d8-46b7-cc05-90b7422d6ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CNQs319EcqMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# used for only one m4a file\n"
      ],
      "metadata": {
        "id": "FPIIQGcRjzQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used for one m4a file\n"
      ],
      "metadata": {
        "id": "MCZJm9h4jsiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load audio file\n",
        "audio_path = \"/content/drive/MyDrive/UMD BIOE CAPSTONE PROJECT/sample/Raw data/SI(ACH000).m4a\"\n",
        "waveform, sample_rate = torchaudio.load(audio_path) # Load audio data and sample rate from Raw data/SI(ACH000) file\n",
        "# waveform represents the audio data that has been loaded from the audio file"
      ],
      "metadata": {
        "id": "9hIKwPKieb1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sample rate\n",
        "if sample_rate != processor.feature_extractor.sampling_rate:\n",
        "    resampler = torchaudio.transforms.Resample(sample_rate, processor.feature_extractor.sampling_rate)\n",
        "    waveform = resampler(waveform)\n",
        "# this part used for checking whether the sample rate of the loaded audio data is equal to the sample rate expected by the processor.\n"
      ],
      "metadata": {
        "id": "6kbIwnF_gYIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format suitable\n",
        "#the code is using the Wav2Vec 2.0 processor (processor) to convert the raw audio waveform (waveform) into a format suitable for input to the Wav2Vec 2.0 model.\n",
        "\n",
        "# used for debug: Print waveform shape for debugging\n",
        "#print(\"Waveform shape:\", waveform.shape)\n",
        "\n",
        "# Ensure mono channel\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "# Ensure correct shape for model input\n",
        "inputs = processor(waveform.squeeze(), sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\")\n",
        "#used squeeze to give right matrix for inpput:torch.Size([1, 4865366])\n",
        "# Print input shape for debugging\n",
        "print(\"Input shape:\", inputs.input_values.shape)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs.input_values)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knFCgkpokKfB",
        "outputId": "b3cdd382-c7cc-4e35-edd1-107f012b199c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 4865366])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. for this one exapmle task. it is one \"No Training Updates:\"\n",
        "which means: During inference, the model's parameters are fixed and not updated.\n",
        "2. in this application of the model , that gradient calculations and parameter updates, which are necessary during training, are not needed. to realize this , torch.no_grad()are used;\n",
        "3.  torch.no_grad()  is a context manager provided by PyTorch. It temporarily sets all the requires_grad flags to False, which disables gradient calculation. This is useful during inference because we don't need gradients and it helps to save memory and improve performence.\n"
      ],
      "metadata": {
        "id": "TmxnTwlHljNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the last hidden states\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "# Print shape of last_hidden_states\n",
        "print(\"Last hidden states shape:\", last_hidden_states.shape)\n",
        "\n",
        "# Print the last_hidden_states\n",
        "print(\"Last hidden states:\", last_hidden_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_FYxn1gruemE",
        "outputId": "6456466f-82e8-4ab2-8af1-41bd1fc99e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'outputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0aa82571519a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the last hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Print shape of last_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Last hidden states shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "last hidden states is final hidden state of a model after processing an input sequence.\n",
        "in this task, the  last hidden states is our expected transcript form example m4a file"
      ],
      "metadata": {
        "id": "R2i10GM8vHrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# use only for seveal m4a file(don't used for only one file)"
      ],
      "metadata": {
        "id": "N8x8H80UihrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code below should only run if these are several m4a file in the address:\n",
        "\"/content/drive/MyDrive/UMD BIOE CAPSTONE PROJECT/sample/Raw data/\"\n",
        "\n",
        "the code also including the smapling ckeck part"
      ],
      "metadata": {
        "id": "ZSisr45Iiuc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Load Wav2Vec 2.0 processor and model\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Directory containing audio files\n",
        "directory = \"/content/drive/MyDrive/UMD BIOE CAPSTONE PROJECT/sample/Raw data/\"\n",
        "\n",
        "# Loop over each file in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".m4a\"):\n",
        "        audio_path = os.path.join(directory, filename)\n",
        "\n",
        "        # Load audio file\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Preprocess the audio (resampling if necessary)\n",
        "        if sample_rate != processor.feature_extractor.sampling_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, processor.feature_extractor.sampling_rate)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Process the audio using the processor\n",
        "        inputs = processor(waveform, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "        # Perform inference using the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Get the last hidden states\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        print(f\"Processed audio file: {audio_path}\")"
      ],
      "metadata": {
        "id": "eBa_n_RpivQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from fairseq.models.wav2vec import Wav2VecModel\n",
        "\n",
        "# Load the pretrained model\n",
        "model = Wav2VecModel.from_pretrained('path_to_pretrained_model')\n",
        "\n",
        "# Load your audio file\n",
        "audio_input, _ = torchaudio.load('path_to_your_audio_file')\n",
        "\n",
        "# Preprocess the audio (resampling, normalization, etc.)\n",
        "# Ensure the audio matches the requirements of the pretrained model\n",
        "\n",
        "# Transcribe the audio\n",
        "transcription = model.transcribe(audio_input)\n",
        "\n",
        "print(\"Transcription:\", transcription)"
      ],
      "metadata": {
        "id": "5AdK0y0OdfED"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}