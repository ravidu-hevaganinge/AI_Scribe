{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial used to write the code below\n",
    "# https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount the google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "!pip install datasets==1.18.3\n",
    "!pip install transformers==4.17.0\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "# audio file path\n",
    "audio_path = \"/content/drive/MyDrive/UMD BIOE CAPSTONE PROJECT/sample/Raw data/SI(ACH000).m4a\"\n",
    "AUDIO_FILE = download_asset(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "\n",
    "print(\"Labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bundle.get_model().to(device)\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(AUDIO_FILE)\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features, _ = model.extract_features(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(features), 1, figsize=(16, 34.4 * len(features)))\n",
    "for i, feats in enumerate(features):\n",
    "    ax[i].imshow(feats[0].cpu(), interpolation=\"nearest\")\n",
    "    ax[i].set_title(f\"Feature from transformer layer {i+1}\")\n",
    "    ax[i].set_xlabel(\"Feature dimension\")\n",
    "    ax[i].set_ylabel(\"Frame (time-axis)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission, _ = model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(emission[0].cpu().T, interpolation=\"nearest\")\n",
    "plt.title(\"Classification result\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "print(\"Class labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor) -> str:\n",
    "        \"\"\"Given a sequence emission over labels, get the best path string\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          str: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        return \"\".join([self.labels[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
    "transcript = decoder(emission[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcript)\n",
    "IPython.display.Audio(AUDIO_FILE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
